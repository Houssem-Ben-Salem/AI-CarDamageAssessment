{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cretiMWlLrvB",
        "outputId": "a3b7d3a4-ba49-4284-be4b-2a848d754bac"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-9Z6YbhLU54"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import clip\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76iTQMw6R1Wv"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
        "model, preprocess = clip.load(\"RN50\",device=device,jit=False) #Must set jit=False for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm2PBbK8LaiA"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, list_image_path, list_txt, transform=None):\n",
        "        # Prepend the path to each image file name\n",
        "        base_path = \"/home/hous/Desktop/LLAVA/CarDD_release/CarDD_COCO/data/\"\n",
        "        self.image_path = [base_path + file_name for file_name in list_image_path]\n",
        "        self.title = clip.tokenize(list_txt)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_path[idx]).convert(\"RGB\")  # Convert to RGB\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        title = self.title[idx].to(device)\n",
        "        return image, title\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIn-99uWMFeh"
      },
      "outputs": [],
      "source": [
        "# Read data from CSV file\n",
        "csv_file = 'pair.csv'  # Replace with your CSV file path\n",
        "data = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.transforms import RandomCrop, GaussianBlur, RandomGrayscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDW3DgYOMOqv"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 2\n",
        "# Split data into training and validation\n",
        "train_data, val_data = train_test_split(data, test_size=0.2)  # 20% for validation\n",
        "resize_size = 224\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((resize_size + 20, resize_size + 20)),  # slightly bigger for cropping\n",
        "    transforms.RandomCrop(resize_size),  # random cropping\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
        "    transforms.RandomApply([GaussianBlur(kernel_size=3)], p=0.2),  # apply Gaussian Blur with a probability of 0.2\n",
        "    transforms.RandomGrayscale(p=0.2),  # convert to grayscale with a probability of 0.2\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = ImageTextDataset(train_data['file_name'].tolist(), train_data['damages'].tolist(), transform=transform)\n",
        "val_dataset = ImageTextDataset(val_data['file_name'].tolist(), val_data['damages'].tolist(), transform=transforms.Compose([\n",
        "    transforms.Resize((resize_size, resize_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
        "]))\n",
        "\n",
        "\n",
        "# Create DataLoaders for both datasets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTXmJxOqMSM1"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate the model on validation data\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images, texts = batch\n",
        "            images = images.to(device)\n",
        "            texts = texts.to(device)\n",
        "            logits_per_image, logits_per_text = model(images, texts)\n",
        "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
        "            total_loss += (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)).item()\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfaKdUftMkUk"
      },
      "outputs": [],
      "source": [
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        p.grad.data = p.grad.data.float()\n",
        "\n",
        "\n",
        "if device == \"cpu\":\n",
        "  model.float()\n",
        "else :\n",
        "  clip.model.convert_weights(model)\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.98), eps=1e-6, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.3, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "\n",
        "# Training loop\n",
        "EPOCHS = 100  # Set the number of epochs\n",
        "best_loss = float('inf')\n",
        "start_epoch = 0  # Default start epoch\n",
        "\n",
        "# Check if a saved model exists\n",
        "saved_model_path = 'best_matching_model.pth'\n",
        "if os.path.isfile(saved_model_path):\n",
        "    checkpoint = torch.load(saved_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    best_loss = checkpoint['best_loss']\n",
        "    print(f\"Resuming training from epoch {start_epoch} with best validation loss {best_loss}\")\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Adding tqdm for training progress\n",
        "    train_loop = tqdm(train_dataloader, total=len(train_dataloader), leave=True)\n",
        "    for images, texts in train_loop:\n",
        "        train_loop.set_description(f'Epoch {epoch+1}/{EPOCHS} [Training]')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits_per_image, logits_per_text = model(images, texts)\n",
        "        ground_truth = torch.arange(images.size(0), device=device)\n",
        "        total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += total_loss.item()\n",
        "\n",
        "        train_loop.set_postfix(train_loss=total_loss.item())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Progress for validation\n",
        "    val_loss = evaluate_model(model, val_dataloader)\n",
        "    tqdm.write(f'Epoch {epoch+1}/{EPOCHS}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Save the best model along with epoch and optimizer state\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state': model.state_dict(),\n",
        "            'optimizer_state': optimizer.state_dict(),\n",
        "            'best_loss': best_loss\n",
        "        }, saved_model_path)\n",
        "\n",
        "print(\"Training completed.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
